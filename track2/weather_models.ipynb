{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2089458e",
   "metadata": {},
   "source": [
    "# Weather forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cbec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from era5.dataloader_era5 import *\n",
    "\n",
    "from aurora import Aurora, rollout\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "from makani.models.networks.sfnonet import SphericalFourierNeuralOperatorNet as SFNO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035e748",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e4f0f",
   "metadata": {},
   "source": [
    "We created a dataloader for loading the ERA5 input data. Each weather forecasting model we include in our setup here is trained on ERA5 data, but needs to have a specific input format. The dataloader loads the data and provides it, depending on which model is used, in the correct format. The dataloader has to be initialized at the beginning, and then you can pick a date and a model, and it outputs the corresponding data at that time in the correct format. Use the ```get_data``` method for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac073b0",
   "metadata": {},
   "source": [
    "We've created a custom dataloader to streamline the process of preparing ERA5 input data for our weather forecasting models.\n",
    "\n",
    "### How It Works\n",
    "This dataloader is designed to handle the specific input format requirements of each model in our setup. Instead of manually reformatting the data for every model, you simply initialize the dataloader once at the start of your program.\n",
    "\n",
    "After initialization, you can use the ```get_data``` method. This method takes two arguments: a date and the specific model you wish to use. The dataloader then automatically fetches the corresponding ERA5 data and delivers it in the correct format for that particular model. This ensures a consistent and efficient data pipeline for the included models. You might want to extend the functionality of the dataloader to other model's requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats for now\n",
    "stats_mean_path = \"/era5/stats_era5/global_means.npy\"\n",
    "stats_std_path = \"/era5/stats_era5/global_stds.npy\"\n",
    "metadata_path = \"/era5/data.json\"\n",
    "data_path = \"/era5/2018/restricted_3days_2018.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dataloader_era5(\n",
    "    data_path=data_path,\n",
    "    stats_mean_path=stats_mean_path,\n",
    "    stats_std_path=stats_std_path,\n",
    "    metadata_path=metadata_path,\n",
    "    in_channels=None,\n",
    "    out_channels=None,\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a61e6",
   "metadata": {},
   "source": [
    "# AURORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668b374",
   "metadata": {},
   "source": [
    "This is an example code of how to run inference for the pretrained AURORA model.\n",
    "\n",
    "- [Paper Link](https://www.nature.com/articles/s41586-025-09005-y)\n",
    "- [Github Link](https://github.com/microsoft/aurora/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "aurora_model = Aurora(use_lora=False)  # The pretrained version does not use LoRA.\n",
    "aurora_model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\")\n",
    "\n",
    "aurora_model.eval()\n",
    "aurora_model = aurora_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"date = \"2018-01-02T06:00:00\"\n",
    "batch = dataloader.get_data(date, model=\"aurora\")\n",
    "#print(f\"Shape of data: {data}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef240307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"aurora_model.eval()\n",
    "aurora_model = aurora_model.to(\"cpu\")\n",
    "batch = batch.to(\"cpu\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = [pred.to(\"cpu\") for pred in rollout(aurora_model, batch, steps=1)]\n",
    "    aurora_model = aurora_model.to(\"cpu\")\n",
    "\n",
    "print(f\"Preds: {preds}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9e8f5",
   "metadata": {},
   "source": [
    "# PANGU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133caa0",
   "metadata": {},
   "source": [
    "This is an example code of how to run inference for the pretrained Pangu-Weather model.\n",
    "\n",
    "- [Paper Link](https://www.nature.com/articles/s41586-023-06185-3)\n",
    "- [Github Link](https://github.com/198808xc/Pangu-Weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pangu_model_sess = pangu.define_pangu_onnx(model_type=6)\n",
    "print(f\"Pangu model {pangu_model_sess}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2018-01-02T06:00:00\"\n",
    "upper_data, surface_data = dataloader.get_data(date, model=\"pangu\")\n",
    "print(f\"Shape of upper data: {upper_data.shape}\")\n",
    "print(f\"Shape of surface data: {surface_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference session\n",
    "output_upper, output_surface = pangu_model_sess.run(None, {'input':upper_data, 'input_surface':surface_data})\n",
    "print(f\"Shape of output_upper: {output_upper.shape}\")\n",
    "print(f\"Shape of output_surface: {output_surface.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64832af7",
   "metadata": {},
   "source": [
    "# SFNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb748d3a",
   "metadata": {},
   "source": [
    "This is an example code of how to run inference for the pretrained SFNO model.\n",
    "\n",
    "- [Paper Link](https://arxiv.org/abs/2306.03838)\n",
    "- [Github Link](https://github.com/NVIDIA/makani/tree/v0.1.1)\n",
    "\n",
    "The model weights were taken from the public NVIDIA Modulus release [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/models/sfno_73ch_small). Note that this is not the final model, but a smaller version of the SFNO model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72752bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2018-01-02T06:00:00\"\n",
    "output_data = dataloader.get_data(date, model=\"sfno\")\n",
    "print(f\"Shape of upper data: {output_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/bids_weather_forecasting_hackathon/checkpoints/sfno_checkpoints/sfno_73ch_small_config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "nlat = config['img_shape_x']\n",
    "nlon = config['img_shape_y']\n",
    "print(f\"num lat: {nlat}\")\n",
    "print(f\"num lon: {nlon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = partial(SFNO, \n",
    "    img_size=(nlat, nlon),  \n",
    "    grid=config[\"data_grid_type\"],\n",
    "    num_layers=config['num_layers'], \n",
    "    scale_factor=config['scale_factor'],\n",
    "    inp_chans=config[\"N_in_channels\"],\n",
    "    out_chans=config[\"N_out_channels\"],\n",
    "    embed_dim=config['embed_dim'], \n",
    "    big_skip=True, \n",
    "    pos_embed=config[\"pos_embed\"], \n",
    "    use_mlp=config[\"use_mlp\"], \n",
    "    normalization_layer=config[\"normalization_layer\"]\n",
    ")\n",
    "\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_prefix_from_state_dict(\n",
    "    state_dict: Dict[str, Any],\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    r\"\"\"Append the prefix to states in state_dict in place.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a local model, a DP/DDP model can load it by applying\n",
    "        `prepend_prefix_to_state_dict(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = list(state_dict.keys())\n",
    "    for key in keys:\n",
    "        # find prefix part in key and remove it\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix):]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if hasattr(state_dict, \"_metadata\"):\n",
    "        keys = list(state_dict._metadata.keys())\n",
    "        for key in keys:\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "            if len(key) >= 0:\n",
    "                newkey = prefix + key\n",
    "                state_dict._metadata[newkey] = state_dict._metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "ckpt_path = \"/bids_weather_forecasting_hackathon/checkpoints/sfno_checkpoints/checkpoints/sfno_73ch_small_training_checkpoints_best_ckpt_mp0.tar\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "state_dict = checkpoint[\"model_state\"]\n",
    "pop_prefix_from_state_dict(state_dict, \"module.model.\")\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(output_data)\n",
    "print(f\"Shape of output: {pred.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
