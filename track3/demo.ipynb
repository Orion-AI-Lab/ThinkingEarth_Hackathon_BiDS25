{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q webdataset matplotlib open_clip_torch img2dataset peft transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and reconstruct the GAIA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Orion-AI-Lab/GAIA\n",
    "# https://huggingface.co/datasets/azavras/GAIA\n",
    "\n",
    "!huggingface-cli download azavras/GAIA --repo-type dataset --local-dir GAIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!img2dataset --url_list \"GAIA_test_data.json\" \\\n",
    "             --input_format \"json\" \\\n",
    "             --url_col \"image_src\" \\\n",
    "             --caption_col \"image_alt\" \\\n",
    "             --output_format \"webdataset\" \\\n",
    "             --save_additional_columns \"['id','captions']\" \\\n",
    "             --output_folder \"test/\" \\\n",
    "             --processes_count 4 \\\n",
    "             --thread_count 4 \\\n",
    "             --retries=5 \\\n",
    "             --image_size 512 \\\n",
    "             --encode_format \"png\" \\\n",
    "             --encode_quality 9 \\\n",
    "             --resize_mode \"keep_ratio\" \\\n",
    "             --number_sample_per_shard 512 \\\n",
    "             --disallowed_header_directives '[]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import webdataset as wds\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"openai\",\n",
    "    device=device,\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32-quickgelu\")\n",
    "\n",
    "# Load first image from test dataset\n",
    "dataset = (\n",
    "    wds.WebDataset(\n",
    "        \"/mnt/nvme1/azavras/GAIA/wds/test/{00000..00015}.tar\", shardshuffle=False\n",
    "    )\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"png\", \"txt\", \"json\")\n",
    ")\n",
    "\n",
    "# Get first image\n",
    "image, text, metadata = next(iter(dataset))\n",
    "print(text)\n",
    "\n",
    "# Preprocess image for model\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Define some common classes to classify against\n",
    "classes = [\n",
    "    \"a satellite image of Europe\",\n",
    "    \"a satellite image of Africa\",\n",
    "    \"a satellite image of Asia\",\n",
    "    \"a satellite image of South America\",\n",
    "    \"a satellite image of North America\",\n",
    "    \"a satellite image of Australia\",\n",
    "    \"a satellite image of Antarctica\",\n",
    "]\n",
    "\n",
    "# Encode image\n",
    "with torch.no_grad():\n",
    "    image_features = F.normalize(model.encode_image(image_input), dim=-1)\n",
    "\n",
    "    # Encode text classes\n",
    "    text_tokens = tokenizer(classes).to(device)\n",
    "    text_features = F.normalize(model.encode_text(text_tokens), dim=-1)\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Display image and top predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Show image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Show predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "y_pos = range(len(values))\n",
    "plt.barh(y_pos, values.cpu().numpy())\n",
    "plt.yticks(y_pos, [classes[i] for i in indices])\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.title(\"Top 5 Predictions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import webdataset as wds\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"openai\",\n",
    "    device=device,\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32-quickgelu\")\n",
    "\n",
    "# Load and process test dataset\n",
    "dataset = (\n",
    "    wds.WebDataset(\"/mnt/nvme1/azavras/GAIA/wds/test/{00000..00015}.tar\")\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"png\", \"txt\", \"json\")\n",
    ")\n",
    "\n",
    "# Process all images\n",
    "all_images = []\n",
    "all_raw_images = []  # Store raw PIL images for display\n",
    "for images, _, _ in dataset:\n",
    "    # Store raw image before preprocessing\n",
    "    all_raw_images.append(images)\n",
    "    # Preprocess for model\n",
    "    processed = preprocess(images).unsqueeze(0)\n",
    "    all_images.append(processed)\n",
    "\n",
    "# Encode all images\n",
    "images = torch.cat(all_images).to(device)\n",
    "with torch.no_grad():\n",
    "    image_embeds = F.normalize(model.encode_image(images), dim=-1)\n",
    "\n",
    "# Example text query\n",
    "text_query = \"a satellite image of a river\"\n",
    "\n",
    "# Encode text query\n",
    "text_tokens = tokenizer([text_query]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embed = F.normalize(model.encode_text(text_tokens), dim=-1)\n",
    "\n",
    "# Compute similarities and get top 5\n",
    "scores = text_embed @ image_embeds.T\n",
    "top_scores, top_indices = scores[0].topk(5)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, (score, idx) in enumerate(zip(top_scores, top_indices), 1):\n",
    "    plt.subplot(1, 5, i)\n",
    "    plt.imshow(all_raw_images[idx])\n",
    "    plt.title(f\"Score: {score:.3f}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(f\"Top 5 images for query: '{text_query}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import webdataset as wds\n",
    "from peft import PeftModel\n",
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor\n",
    "\n",
    "\n",
    "# Initialize BLIP-2 model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load processor from original model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Load base model\n",
    "base_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    ")\n",
    "base_model.to(device)\n",
    "\n",
    "# Load fine-tuned adapter\n",
    "checkpoint_path = \"...\"\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# Load first image from test dataset\n",
    "dataset = (\n",
    "    wds.WebDataset(\"/mnt/nvme1/azavras/GAIA/wds/test/{00000..00015}.tar\")\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"png\", \"txt\", \"json\")\n",
    ")\n",
    "\n",
    "# Get first image\n",
    "image, text, metadata = next(iter(dataset))\n",
    "\n",
    "# Process image for BLIP-2\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=120,\n",
    "        num_beams=5,\n",
    "    )\n",
    "\n",
    "# Decode caption\n",
    "caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# Display image with caption\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\n",
    "    f\"Ground truth caption: {metadata['captions'][0]}\\n\\nGenerated Caption: {caption}\",\n",
    "    wrap=True,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
